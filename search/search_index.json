{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction doe is a python package for the computation of (D-)optimal experimental designs. It uses opti for experiment specification and adding domain knowledge and formulaic . Opti allows to define an arbitrary number of decision variables using Problem objects. These can take values corresponding to their type and domain, e.g. continuous: \\(x_1 \\in [0, 1]\\) discrete: \\(x_2 \\in \\{1, 2, 5, 7.5\\}\\) categorical: \\(x_3 \\in \\{A, B, C\\}\\) . Warning Discrete and categorical variables cannot currently be constrained. Additionally, constraints on the values of the decision variables can be taken into account, e.g. linear equality: \\(\\sum x_i = 1\\) linear inequality: \\(2 x_1 \\leq x_2\\) non-linear equality: \\(\\sum x_i^2 = 1\\) non-linear inequality: \\(\\sum x_i^2 \\leq 1\\) n-choose-k: only \\(k\\) out of \\(n\\) parameters can take non-zero values. The model to be fitted can be specified using formulaic's Formula objects, strings following Wilkinson notation or - with the context of the problem specification - using certain keywords like \"linear\" or \"fully-quadratic\" .","title":"Overview"},{"location":"#introduction","text":"doe is a python package for the computation of (D-)optimal experimental designs. It uses opti for experiment specification and adding domain knowledge and formulaic . Opti allows to define an arbitrary number of decision variables using Problem objects. These can take values corresponding to their type and domain, e.g. continuous: \\(x_1 \\in [0, 1]\\) discrete: \\(x_2 \\in \\{1, 2, 5, 7.5\\}\\) categorical: \\(x_3 \\in \\{A, B, C\\}\\) . Warning Discrete and categorical variables cannot currently be constrained. Additionally, constraints on the values of the decision variables can be taken into account, e.g. linear equality: \\(\\sum x_i = 1\\) linear inequality: \\(2 x_1 \\leq x_2\\) non-linear equality: \\(\\sum x_i^2 = 1\\) non-linear inequality: \\(\\sum x_i^2 \\leq 1\\) n-choose-k: only \\(k\\) out of \\(n\\) parameters can take non-zero values. The model to be fitted can be specified using formulaic's Formula objects, strings following Wilkinson notation or - with the context of the problem specification - using certain keywords like \"linear\" or \"fully-quadratic\" .","title":"Introduction"},{"location":"install/","text":"Install pyreto can be installed from nexus.roqs.basf.net . pip install basf-doe Please make sure to have cyipopt installed. On Windows with conda the easiest way to get this package is using conda install -c conda-forge cyipopt See this link for more information on other ways to install cyipopt.","title":"Install"},{"location":"install/#install","text":"pyreto can be installed from nexus.roqs.basf.net . pip install basf-doe Please make sure to have cyipopt installed. On Windows with conda the easiest way to get this package is using conda install -c conda-forge cyipopt See this link for more information on other ways to install cyipopt.","title":"Install"},{"location":"large_mixture/","text":"Large decision variable space In this example we demonstrate the capability of doe for models with a large number of model terms. We consider a problem with 20 decision variables, only one mixture constraint and a fully quadratic model. In this case the model has 231 terms. Modde is not able to generate a design matrix for this problem. With doe you can find a design for this problem with the following code. Please note that for large problems the optimization can take quite some time! Here, we stop the optimization after 200 iterations. import opti from doe import find_local_max_ipopt problem = opti . Problem ( inputs = [ opti . Continuous ( f \"x { i + 1 } \" , [ 0 , 1 ]) for i in range ( 20 )], outputs = [ opti . Continuous ( \"y\" )], constraints = [ opti . LinearEquality ( names = [ f \"x { i + 1 } \" for i in range ( 20 )], rhs = 1 )], ) res = find_local_max_ipopt ( problem = problem , model_type = \"fully-quadratic\" , ipopt_options = { \"maxiter\" : 200 }) We obtain the following results. x1 x2 x3 x4 x5 x6 x7 x8 x9 x10 x11 x12 x13 x14 x15 x16 x17 x18 x19 x20 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.5 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.5 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.5 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.5 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.5 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.5 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.5 0.0 0.3 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.3 0.3 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.5 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.5 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.2 0.0 0.0 0.0 0.2 0.0 0.2 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.2 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.3 0.0 0.0 0.0 0.3 0.0 0.3 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0","title":"Large decision variable space"},{"location":"large_mixture/#large-decision-variable-space","text":"In this example we demonstrate the capability of doe for models with a large number of model terms. We consider a problem with 20 decision variables, only one mixture constraint and a fully quadratic model. In this case the model has 231 terms. Modde is not able to generate a design matrix for this problem. With doe you can find a design for this problem with the following code. Please note that for large problems the optimization can take quite some time! Here, we stop the optimization after 200 iterations. import opti from doe import find_local_max_ipopt problem = opti . Problem ( inputs = [ opti . Continuous ( f \"x { i + 1 } \" , [ 0 , 1 ]) for i in range ( 20 )], outputs = [ opti . Continuous ( \"y\" )], constraints = [ opti . LinearEquality ( names = [ f \"x { i + 1 } \" for i in range ( 20 )], rhs = 1 )], ) res = find_local_max_ipopt ( problem = problem , model_type = \"fully-quadratic\" , ipopt_options = { \"maxiter\" : 200 }) We obtain the following results. x1 x2 x3 x4 x5 x6 x7 x8 x9 x10 x11 x12 x13 x14 x15 x16 x17 x18 x19 x20 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.5 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.5 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.5 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.5 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.5 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.5 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.5 0.0 0.3 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.3 0.3 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.5 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.5 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.2 0.0 0.0 0.0 0.2 0.0 0.2 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.2 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.3 0.0 0.0 0.0 0.3 0.0 0.3 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0","title":"Large decision variable space"},{"location":"modde_comparison/","text":"Performance compared to modde The purpose of this package is enable the user to find D-optimal designs in the most general setting. doe can be used for larger models (e.g. this example ) with more general constraints like NChooseK constraints (see here ) or multiple mixture constraints (see here ). Unfortunately, the performance is decreased compared to modde in cases where modde can actually find a design. To see this, one can determine models for the following problems using doe. Problem 1 problem = opti . Problem ( inputs = [ opti . Continuous ( f \"x { i + 1 } \" , [ 0 , 1 ]) for i in range ( 12 )] + [ opti . Continuous ( f \"p { i + 1 } \" , [ - 1 , 1 ]) for i in range ( 3 )], outputs = [ opti . Continuous ( \"y\" )], constraints = [ opti . LinearEquality ( names = [ f \"x { i + 1 } \" for i in range ( 12 )], rhs = 1 )], ) Problem 2 problem = opti . Problem ( inputs = [ opti . Continuous ( f \"x { i + 1 } \" , [ 0 , 1 ]) for i in range ( 12 )] + [ opti . Continuous ( f \"p { i + 1 } \" , [ - 1 , 1 ]) for i in range ( 3 )], outputs = [ opti . Continuous ( \"y\" )], constraints = [ opti . LinearEquality ( names = [ f \"x { i + 1 } \" for i in range ( 12 )], rhs = 1 ), opti . LinearInequality ( names = [ \"x1\" , \"x2\" , \"x3\" ], rhs = 0.7 ), opti . LinearInequality ( names = [ \"x1\" , \"x2\" , \"x3\" ], lhs =- 1 , rhs =- 0.3 ), opti . LinearInequality ( names = [ \"x10\" , \"x11\" , \"x12\" ], rhs = 0.8 ), opti . LinearInequality ( names = [ \"x10\" , \"x11\" , \"x12\" ], lhs =- 1 , rhs =- 0.2 ), opti . LinearInequality ( names = [ \"x5\" , \"x6\" ], lhs = [ - 1 , 0.5 ], rhs = 0 ), opti . LinearInequality ( names = [ \"x5\" , \"x6\" ], lhs = [ - 1 , 2 ], rhs = 0 ), ], ) The diagram below shows the D optimality for the modde result, the doe result and a random design (as a reference).","title":"Performance compared to modde"},{"location":"modde_comparison/#performance-compared-to-modde","text":"The purpose of this package is enable the user to find D-optimal designs in the most general setting. doe can be used for larger models (e.g. this example ) with more general constraints like NChooseK constraints (see here ) or multiple mixture constraints (see here ). Unfortunately, the performance is decreased compared to modde in cases where modde can actually find a design. To see this, one can determine models for the following problems using doe.","title":"Performance compared to modde"},{"location":"modde_comparison/#problem-1","text":"problem = opti . Problem ( inputs = [ opti . Continuous ( f \"x { i + 1 } \" , [ 0 , 1 ]) for i in range ( 12 )] + [ opti . Continuous ( f \"p { i + 1 } \" , [ - 1 , 1 ]) for i in range ( 3 )], outputs = [ opti . Continuous ( \"y\" )], constraints = [ opti . LinearEquality ( names = [ f \"x { i + 1 } \" for i in range ( 12 )], rhs = 1 )], )","title":"Problem 1"},{"location":"modde_comparison/#problem-2","text":"problem = opti . Problem ( inputs = [ opti . Continuous ( f \"x { i + 1 } \" , [ 0 , 1 ]) for i in range ( 12 )] + [ opti . Continuous ( f \"p { i + 1 } \" , [ - 1 , 1 ]) for i in range ( 3 )], outputs = [ opti . Continuous ( \"y\" )], constraints = [ opti . LinearEquality ( names = [ f \"x { i + 1 } \" for i in range ( 12 )], rhs = 1 ), opti . LinearInequality ( names = [ \"x1\" , \"x2\" , \"x3\" ], rhs = 0.7 ), opti . LinearInequality ( names = [ \"x1\" , \"x2\" , \"x3\" ], lhs =- 1 , rhs =- 0.3 ), opti . LinearInequality ( names = [ \"x10\" , \"x11\" , \"x12\" ], rhs = 0.8 ), opti . LinearInequality ( names = [ \"x10\" , \"x11\" , \"x12\" ], lhs =- 1 , rhs =- 0.2 ), opti . LinearInequality ( names = [ \"x5\" , \"x6\" ], lhs = [ - 1 , 0.5 ], rhs = 0 ), opti . LinearInequality ( names = [ \"x5\" , \"x6\" ], lhs = [ - 1 , 2 ], rhs = 0 ), ], ) The diagram below shows the D optimality for the modde result, the doe result and a random design (as a reference).","title":"Problem 2"},{"location":"multiple_mixture_constraints/","text":"Multiple mixture constraints In this example we want to obtain a design with 125 experiments for the following problem with two mixture constraints problem = opti . Problem ( inputs = [ opti . Continuous ( f \"x { i + 1 } \" , [ 0 , 1 ]) for i in range ( 12 )] + [ opti . Continuous ( f \"p { i + 1 } \" , [ - 1 , 1 ]) for i in range ( 3 )], outputs = [ opti . Continuous ( \"y\" )], constraints = [ opti . LinearEquality ( names = [ f \"x { i + 1 } \" for i in range ( 6 )], rhs = 1 ), opti . LinearEquality ( names = [ f \"x { i + 1 } \" for i in range ( 6 , 12 )], rhs = 1 ), ], ) res = find_local_max_ipopt ( problem = problem , model_type = \"fully-quadratic\" , n_experiments = 125 , ipopt_options = { \"maxiter\" : 2000 }, ) We obtain the following design. x1 x2 x3 x4 x5 x6 x7 x8 x9 x10 x11 x12 p1 p2 p3 0.56 0.44 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.29 -1.00 -1.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 -0.47 -1.00 -1.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.42 0.59 0.00 -1.00 -1.00 -1.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 1.00 0.00 0.00 0.00 -1.00 1.00 -1.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 -1.00 1.00 -1.00 0.00 0.00 0.38 0.00 0.62 0.00 0.51 0.00 0.00 0.00 0.50 0.00 -1.00 0.12 0.23 0.00 1.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 -1.00 1.00 -1.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 0.09 -1.00 -1.00 0.00 0.00 0.19 0.81 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 -1.00 -1.00 -1.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 1.00 0.00 -1.00 -1.00 -1.00 0.51 0.00 0.50 0.00 0.00 0.00 0.00 0.48 0.00 0.52 0.00 0.00 -1.00 0.08 -1.00 0.00 0.00 0.47 0.53 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 -1.00 1.00 -1.00 0.00 0.50 0.00 0.00 0.00 0.50 0.00 0.52 0.00 0.00 0.00 0.48 -1.00 1.00 -1.00 0.00 0.89 0.00 0.10 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 -1.00 1.00 1.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 1.00 1.00 -1.00 0.00 0.00 0.00 0.58 0.00 0.42 0.00 0.00 1.00 0.00 0.00 0.00 1.00 -1.00 -1.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 -1.00 1.00 1.00 0.00 0.00 0.00 0.56 0.00 0.44 0.00 0.00 0.51 0.00 0.00 0.49 -1.00 1.00 1.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 1.00 0.00 0.00 1.00 1.00 -1.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 1.00 1.00 -1.00 0.41 0.00 0.59 0.00 0.00 0.00 0.00 0.00 0.58 0.42 0.00 0.00 1.00 -1.00 -1.00 0.74 0.00 0.00 0.00 0.00 0.26 0.00 0.73 0.00 0.00 0.27 0.00 0.18 -1.00 -0.92 0.61 0.00 0.39 0.00 0.00 0.00 0.00 0.55 0.45 0.00 0.00 0.00 -1.00 -1.00 1.00 0.00 0.49 0.00 0.00 0.51 0.00 0.00 0.00 0.00 0.47 0.00 0.54 -1.00 -1.00 1.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 -1.00 -1.00 1.00 0.00 0.00 0.50 0.00 0.50 0.00 0.00 0.00 0.00 0.38 0.61 0.00 0.05 1.00 1.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 -1.00 1.00 -0.61 0.48 0.00 0.00 0.00 0.53 0.00 0.00 1.00 0.00 0.00 0.00 0.00 -1.00 -1.00 -1.00 0.54 0.00 0.00 0.00 0.00 0.46 0.44 0.00 0.00 0.00 0.00 0.56 -1.00 -1.00 -1.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 -1.00 -1.00 -1.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.69 0.30 0.00 0.00 0.00 -1.00 1.00 1.00 0.00 0.00 0.44 0.00 0.00 0.56 0.00 0.00 0.43 0.00 0.00 0.57 1.00 1.00 0.25 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 1.00 1.00 1.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 1.00 0.00 0.00 0.00 0.00 -1.00 1.00 1.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 1.00 0.00 1.00 1.00 1.00 0.00 0.00 0.57 0.43 0.00 0.00 0.51 0.00 0.00 0.00 0.49 0.00 -0.27 1.00 1.00 0.59 0.00 0.00 0.41 0.00 0.00 0.47 0.00 0.00 0.00 0.00 0.53 0.01 1.00 1.00 0.00 0.00 0.00 0.00 1.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 1.00 1.00 1.00 0.00 1.00 0.00 0.00 0.00 0.00 0.16 0.00 0.84 0.00 0.00 0.00 -1.00 1.00 1.00 0.00 0.53 0.00 0.00 0.47 0.00 0.00 0.39 0.00 0.61 0.00 0.00 1.00 1.00 -0.49 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 1.00 1.00 -1.00 0.00 0.00 0.00 0.00 0.78 0.22 0.45 0.00 0.00 0.55 0.00 0.00 -1.00 1.00 -1.00 0.00 0.00 0.00 0.60 0.00 0.41 0.00 1.00 0.00 0.00 0.00 0.00 -1.00 1.00 -0.24 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 1.00 1.00 1.00 -1.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.59 0.00 0.41 0.00 1.00 -1.00 1.00 0.00 0.44 0.00 0.56 0.00 0.00 0.00 0.00 0.00 0.53 0.00 0.48 1.00 0.06 -1.00 1.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 1.00 1.00 -1.00 0.54 0.00 0.00 0.00 0.00 0.47 0.00 0.00 0.00 0.48 0.52 0.00 -1.00 1.00 1.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 1.00 -1.00 -1.00 1.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 1.00 -1.00 -1.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 1.00 -1.00 -1.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 -1.00 -1.00 -1.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 1.00 1.00 1.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 1.00 1.00 -1.00 0.00 0.55 0.00 0.00 0.00 0.45 0.00 0.00 0.00 0.00 0.55 0.45 -0.19 1.00 0.24 0.76 0.00 0.00 0.23 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 1.00 -0.15 1.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 1.00 1.00 1.00 0.00 0.00 0.00 1.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 1.00 1.00 -1.00 0.00 1.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 1.00 -1.00 1.00 0.00 0.47 0.54 0.00 0.00 0.00 0.51 0.00 0.49 0.00 0.00 0.00 0.39 -1.00 -1.00 0.00 0.00 0.00 0.52 0.48 0.00 0.54 0.46 0.00 0.00 0.00 0.00 0.15 1.00 -1.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 1.00 -1.00 0.04 0.00 0.64 0.00 0.00 0.00 0.36 0.44 0.00 0.00 0.00 0.00 0.56 1.00 -1.00 -1.00 0.00 0.00 1.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 1.00 0.02 -1.00 0.00 0.00 0.00 0.00 0.53 0.48 0.00 0.50 0.00 0.00 0.00 0.51 1.00 -1.00 1.00 0.00 0.46 0.00 0.54 0.00 0.00 0.00 0.00 0.00 0.48 0.52 0.00 1.00 -1.00 0.50 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 1.00 -1.00 1.00 0.00 0.43 0.57 0.00 0.00 0.00 0.57 0.00 0.00 0.00 0.00 0.43 -1.00 -1.00 1.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.84 -1.00 -0.83 0.45 0.00 0.00 0.00 0.55 0.00 0.48 0.00 0.00 0.53 0.00 0.00 -0.04 -1.00 1.00 0.00 0.54 0.00 0.00 0.46 0.00 0.00 0.58 0.00 0.00 0.42 0.00 1.00 -0.12 1.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 1.00 -1.00 1.00 0.33 0.66 0.00 0.00 0.00 0.00 0.00 0.49 0.00 0.51 0.00 0.00 1.00 -1.00 1.00 0.00 0.56 0.00 0.00 0.00 0.44 0.00 0.00 0.00 1.00 0.00 0.00 1.00 -0.27 1.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 1.00 -1.00 1.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 1.00 -0.09 1.00 0.00 0.00 0.47 0.53 0.00 0.00 0.00 0.50 0.00 0.00 0.00 0.50 1.00 0.64 0.24 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 1.00 -1.00 1.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 1.00 1.00 1.00 0.00 0.00 0.65 0.00 0.35 0.00 0.56 0.00 0.00 0.45 0.00 0.00 1.00 -1.00 -1.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 1.00 0.00 0.00 0.00 1.00 -1.00 1.00 1.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 -1.00 -1.00 1.00 0.00 0.00 0.00 0.00 0.00 1.00 1.00 0.00 0.00 0.00 0.00 0.00 1.00 -1.00 -1.00 0.00 0.00 0.45 0.00 0.55 0.00 0.51 0.00 0.49 0.00 0.00 0.00 1.00 -1.00 1.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 1.00 0.00 0.00 0.00 0.00 1.00 -1.00 -1.00 0.00 0.00 0.00 0.51 0.00 0.49 0.52 0.49 0.00 0.00 0.00 0.00 1.00 -1.00 1.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 1.00 -1.00 1.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 -1.00 1.00 1.00 0.34 0.00 0.00 0.67 0.00 0.00 0.00 0.00 0.54 0.46 0.00 0.00 -1.00 -1.00 -0.01 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.48 0.52 -1.00 -1.00 1.00 0.00 0.00 0.00 1.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 -1.00 -0.35 1.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 1.00 0.00 0.00 -1.00 -1.00 1.00 0.00 0.00 0.73 0.00 0.00 0.27 0.00 0.00 1.00 0.00 0.00 0.00 -1.00 -1.00 1.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 1.00 -1.00 1.00 0.00 0.53 0.47 0.00 0.00 0.00 0.00 0.00 0.00 0.49 0.00 0.51 0.57 1.00 -1.00 0.00 0.61 0.00 0.39 0.00 0.00 0.00 0.00 0.64 0.00 0.36 0.00 -1.00 -1.00 -1.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 -1.00 -0.50 -0.70 0.00 0.00 0.00 0.53 0.46 0.00 0.00 0.00 0.00 0.00 0.49 0.51 1.00 -1.00 -1.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 -1.00 1.00 -1.00 0.00 0.00 0.00 0.00 1.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 -1.00 -1.00 -1.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 -1.00 1.00 -1.00 0.00 0.00 1.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 -1.00 1.00 1.00 0.49 0.00 0.50 0.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 1.00 1.00 1.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 -1.00 -1.00 -1.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 1.00 0.00 0.00 -1.00 1.00 0.04 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.58 0.43 0.00 0.00 0.00 -1.00 0.62 -1.00 0.41 0.59 0.00 0.00 0.00 0.00 0.48 0.00 0.00 0.52 0.00 0.00 1.00 1.00 0.37 0.00 0.00 0.00 0.00 0.45 0.55 0.00 0.00 0.72 0.00 0.28 0.00 -0.32 0.43 1.00 0.00 0.46 0.00 0.54 0.00 0.00 0.00 0.48 0.52 0.00 0.00 0.00 1.00 1.00 1.00 0.00 0.00 0.00 0.00 0.69 0.30 0.00 0.00 0.00 0.47 0.53 0.00 1.00 1.00 1.00 0.77 0.00 0.00 0.23 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 1.00 1.00 -1.00 0.47 0.53 0.00 0.00 0.00 0.00 0.00 0.00 0.53 0.00 0.00 0.47 1.00 1.00 -1.00 0.00 0.00 0.00 0.00 0.56 0.44 0.00 0.49 0.00 0.00 0.50 0.00 1.00 1.00 -1.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 1.00 1.00 -1.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 1.00 1.00 1.00 0.00 0.00 0.00 0.55 0.44 0.00 0.00 1.00 0.00 0.00 0.00 0.00 -1.00 -1.00 1.00 0.00 0.00 0.00 0.00 0.00 1.00 1.00 0.00 0.00 0.00 0.00 0.00 -1.00 1.00 1.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.40 0.60 0.00 0.00 -1.00 1.00 1.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 -1.00 -0.86 1.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 1.00 1.00 0.83 0.49 0.00 0.00 0.51 0.00 0.00 0.46 0.54 0.00 0.00 0.00 0.00 -1.00 1.00 1.00 0.50 0.00 0.00 0.00 0.50 0.00 0.00 0.00 0.00 0.46 0.00 0.54 -1.00 1.00 0.04 0.00 0.00 0.49 0.00 0.00 0.51 0.00 0.00 0.54 0.00 0.46 0.00 1.00 1.00 -1.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 -1.00 -1.00 1.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 -1.00 -1.00 -1.00","title":"Multiple mixture constraints"},{"location":"multiple_mixture_constraints/#multiple-mixture-constraints","text":"In this example we want to obtain a design with 125 experiments for the following problem with two mixture constraints problem = opti . Problem ( inputs = [ opti . Continuous ( f \"x { i + 1 } \" , [ 0 , 1 ]) for i in range ( 12 )] + [ opti . Continuous ( f \"p { i + 1 } \" , [ - 1 , 1 ]) for i in range ( 3 )], outputs = [ opti . Continuous ( \"y\" )], constraints = [ opti . LinearEquality ( names = [ f \"x { i + 1 } \" for i in range ( 6 )], rhs = 1 ), opti . LinearEquality ( names = [ f \"x { i + 1 } \" for i in range ( 6 , 12 )], rhs = 1 ), ], ) res = find_local_max_ipopt ( problem = problem , model_type = \"fully-quadratic\" , n_experiments = 125 , ipopt_options = { \"maxiter\" : 2000 }, ) We obtain the following design. x1 x2 x3 x4 x5 x6 x7 x8 x9 x10 x11 x12 p1 p2 p3 0.56 0.44 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.29 -1.00 -1.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 -0.47 -1.00 -1.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.42 0.59 0.00 -1.00 -1.00 -1.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 1.00 0.00 0.00 0.00 -1.00 1.00 -1.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 -1.00 1.00 -1.00 0.00 0.00 0.38 0.00 0.62 0.00 0.51 0.00 0.00 0.00 0.50 0.00 -1.00 0.12 0.23 0.00 1.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 -1.00 1.00 -1.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 0.09 -1.00 -1.00 0.00 0.00 0.19 0.81 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 -1.00 -1.00 -1.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 1.00 0.00 -1.00 -1.00 -1.00 0.51 0.00 0.50 0.00 0.00 0.00 0.00 0.48 0.00 0.52 0.00 0.00 -1.00 0.08 -1.00 0.00 0.00 0.47 0.53 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 -1.00 1.00 -1.00 0.00 0.50 0.00 0.00 0.00 0.50 0.00 0.52 0.00 0.00 0.00 0.48 -1.00 1.00 -1.00 0.00 0.89 0.00 0.10 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 -1.00 1.00 1.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 1.00 1.00 -1.00 0.00 0.00 0.00 0.58 0.00 0.42 0.00 0.00 1.00 0.00 0.00 0.00 1.00 -1.00 -1.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 -1.00 1.00 1.00 0.00 0.00 0.00 0.56 0.00 0.44 0.00 0.00 0.51 0.00 0.00 0.49 -1.00 1.00 1.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 1.00 0.00 0.00 1.00 1.00 -1.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 1.00 1.00 -1.00 0.41 0.00 0.59 0.00 0.00 0.00 0.00 0.00 0.58 0.42 0.00 0.00 1.00 -1.00 -1.00 0.74 0.00 0.00 0.00 0.00 0.26 0.00 0.73 0.00 0.00 0.27 0.00 0.18 -1.00 -0.92 0.61 0.00 0.39 0.00 0.00 0.00 0.00 0.55 0.45 0.00 0.00 0.00 -1.00 -1.00 1.00 0.00 0.49 0.00 0.00 0.51 0.00 0.00 0.00 0.00 0.47 0.00 0.54 -1.00 -1.00 1.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 -1.00 -1.00 1.00 0.00 0.00 0.50 0.00 0.50 0.00 0.00 0.00 0.00 0.38 0.61 0.00 0.05 1.00 1.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 -1.00 1.00 -0.61 0.48 0.00 0.00 0.00 0.53 0.00 0.00 1.00 0.00 0.00 0.00 0.00 -1.00 -1.00 -1.00 0.54 0.00 0.00 0.00 0.00 0.46 0.44 0.00 0.00 0.00 0.00 0.56 -1.00 -1.00 -1.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 -1.00 -1.00 -1.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.69 0.30 0.00 0.00 0.00 -1.00 1.00 1.00 0.00 0.00 0.44 0.00 0.00 0.56 0.00 0.00 0.43 0.00 0.00 0.57 1.00 1.00 0.25 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 1.00 1.00 1.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 1.00 0.00 0.00 0.00 0.00 -1.00 1.00 1.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 1.00 0.00 1.00 1.00 1.00 0.00 0.00 0.57 0.43 0.00 0.00 0.51 0.00 0.00 0.00 0.49 0.00 -0.27 1.00 1.00 0.59 0.00 0.00 0.41 0.00 0.00 0.47 0.00 0.00 0.00 0.00 0.53 0.01 1.00 1.00 0.00 0.00 0.00 0.00 1.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 1.00 1.00 1.00 0.00 1.00 0.00 0.00 0.00 0.00 0.16 0.00 0.84 0.00 0.00 0.00 -1.00 1.00 1.00 0.00 0.53 0.00 0.00 0.47 0.00 0.00 0.39 0.00 0.61 0.00 0.00 1.00 1.00 -0.49 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 1.00 1.00 -1.00 0.00 0.00 0.00 0.00 0.78 0.22 0.45 0.00 0.00 0.55 0.00 0.00 -1.00 1.00 -1.00 0.00 0.00 0.00 0.60 0.00 0.41 0.00 1.00 0.00 0.00 0.00 0.00 -1.00 1.00 -0.24 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 1.00 1.00 1.00 -1.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.59 0.00 0.41 0.00 1.00 -1.00 1.00 0.00 0.44 0.00 0.56 0.00 0.00 0.00 0.00 0.00 0.53 0.00 0.48 1.00 0.06 -1.00 1.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 1.00 1.00 -1.00 0.54 0.00 0.00 0.00 0.00 0.47 0.00 0.00 0.00 0.48 0.52 0.00 -1.00 1.00 1.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 1.00 -1.00 -1.00 1.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 1.00 -1.00 -1.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 1.00 -1.00 -1.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 -1.00 -1.00 -1.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 1.00 1.00 1.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 1.00 1.00 -1.00 0.00 0.55 0.00 0.00 0.00 0.45 0.00 0.00 0.00 0.00 0.55 0.45 -0.19 1.00 0.24 0.76 0.00 0.00 0.23 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 1.00 -0.15 1.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 1.00 1.00 1.00 0.00 0.00 0.00 1.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 1.00 1.00 -1.00 0.00 1.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 1.00 -1.00 1.00 0.00 0.47 0.54 0.00 0.00 0.00 0.51 0.00 0.49 0.00 0.00 0.00 0.39 -1.00 -1.00 0.00 0.00 0.00 0.52 0.48 0.00 0.54 0.46 0.00 0.00 0.00 0.00 0.15 1.00 -1.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 1.00 -1.00 0.04 0.00 0.64 0.00 0.00 0.00 0.36 0.44 0.00 0.00 0.00 0.00 0.56 1.00 -1.00 -1.00 0.00 0.00 1.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 1.00 0.02 -1.00 0.00 0.00 0.00 0.00 0.53 0.48 0.00 0.50 0.00 0.00 0.00 0.51 1.00 -1.00 1.00 0.00 0.46 0.00 0.54 0.00 0.00 0.00 0.00 0.00 0.48 0.52 0.00 1.00 -1.00 0.50 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 1.00 -1.00 1.00 0.00 0.43 0.57 0.00 0.00 0.00 0.57 0.00 0.00 0.00 0.00 0.43 -1.00 -1.00 1.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.84 -1.00 -0.83 0.45 0.00 0.00 0.00 0.55 0.00 0.48 0.00 0.00 0.53 0.00 0.00 -0.04 -1.00 1.00 0.00 0.54 0.00 0.00 0.46 0.00 0.00 0.58 0.00 0.00 0.42 0.00 1.00 -0.12 1.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 1.00 -1.00 1.00 0.33 0.66 0.00 0.00 0.00 0.00 0.00 0.49 0.00 0.51 0.00 0.00 1.00 -1.00 1.00 0.00 0.56 0.00 0.00 0.00 0.44 0.00 0.00 0.00 1.00 0.00 0.00 1.00 -0.27 1.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 1.00 -1.00 1.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 1.00 -0.09 1.00 0.00 0.00 0.47 0.53 0.00 0.00 0.00 0.50 0.00 0.00 0.00 0.50 1.00 0.64 0.24 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 1.00 -1.00 1.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 1.00 1.00 1.00 0.00 0.00 0.65 0.00 0.35 0.00 0.56 0.00 0.00 0.45 0.00 0.00 1.00 -1.00 -1.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 1.00 0.00 0.00 0.00 1.00 -1.00 1.00 1.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 -1.00 -1.00 1.00 0.00 0.00 0.00 0.00 0.00 1.00 1.00 0.00 0.00 0.00 0.00 0.00 1.00 -1.00 -1.00 0.00 0.00 0.45 0.00 0.55 0.00 0.51 0.00 0.49 0.00 0.00 0.00 1.00 -1.00 1.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 1.00 0.00 0.00 0.00 0.00 1.00 -1.00 -1.00 0.00 0.00 0.00 0.51 0.00 0.49 0.52 0.49 0.00 0.00 0.00 0.00 1.00 -1.00 1.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 1.00 -1.00 1.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 -1.00 1.00 1.00 0.34 0.00 0.00 0.67 0.00 0.00 0.00 0.00 0.54 0.46 0.00 0.00 -1.00 -1.00 -0.01 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.48 0.52 -1.00 -1.00 1.00 0.00 0.00 0.00 1.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 -1.00 -0.35 1.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 1.00 0.00 0.00 -1.00 -1.00 1.00 0.00 0.00 0.73 0.00 0.00 0.27 0.00 0.00 1.00 0.00 0.00 0.00 -1.00 -1.00 1.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 1.00 -1.00 1.00 0.00 0.53 0.47 0.00 0.00 0.00 0.00 0.00 0.00 0.49 0.00 0.51 0.57 1.00 -1.00 0.00 0.61 0.00 0.39 0.00 0.00 0.00 0.00 0.64 0.00 0.36 0.00 -1.00 -1.00 -1.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 -1.00 -0.50 -0.70 0.00 0.00 0.00 0.53 0.46 0.00 0.00 0.00 0.00 0.00 0.49 0.51 1.00 -1.00 -1.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 -1.00 1.00 -1.00 0.00 0.00 0.00 0.00 1.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 -1.00 -1.00 -1.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 -1.00 1.00 -1.00 0.00 0.00 1.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 -1.00 1.00 1.00 0.49 0.00 0.50 0.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 1.00 1.00 1.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 -1.00 -1.00 -1.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 1.00 0.00 0.00 -1.00 1.00 0.04 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.58 0.43 0.00 0.00 0.00 -1.00 0.62 -1.00 0.41 0.59 0.00 0.00 0.00 0.00 0.48 0.00 0.00 0.52 0.00 0.00 1.00 1.00 0.37 0.00 0.00 0.00 0.00 0.45 0.55 0.00 0.00 0.72 0.00 0.28 0.00 -0.32 0.43 1.00 0.00 0.46 0.00 0.54 0.00 0.00 0.00 0.48 0.52 0.00 0.00 0.00 1.00 1.00 1.00 0.00 0.00 0.00 0.00 0.69 0.30 0.00 0.00 0.00 0.47 0.53 0.00 1.00 1.00 1.00 0.77 0.00 0.00 0.23 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 1.00 1.00 -1.00 0.47 0.53 0.00 0.00 0.00 0.00 0.00 0.00 0.53 0.00 0.00 0.47 1.00 1.00 -1.00 0.00 0.00 0.00 0.00 0.56 0.44 0.00 0.49 0.00 0.00 0.50 0.00 1.00 1.00 -1.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 1.00 1.00 -1.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 1.00 1.00 1.00 0.00 0.00 0.00 0.55 0.44 0.00 0.00 1.00 0.00 0.00 0.00 0.00 -1.00 -1.00 1.00 0.00 0.00 0.00 0.00 0.00 1.00 1.00 0.00 0.00 0.00 0.00 0.00 -1.00 1.00 1.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.40 0.60 0.00 0.00 -1.00 1.00 1.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 -1.00 -0.86 1.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 1.00 1.00 0.83 0.49 0.00 0.00 0.51 0.00 0.00 0.46 0.54 0.00 0.00 0.00 0.00 -1.00 1.00 1.00 0.50 0.00 0.00 0.00 0.50 0.00 0.00 0.00 0.00 0.46 0.00 0.54 -1.00 1.00 0.04 0.00 0.00 0.49 0.00 0.00 0.51 0.00 0.00 0.54 0.00 0.46 0.00 1.00 1.00 -1.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 -1.00 -1.00 1.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 -1.00 -1.00 -1.00","title":"Multiple mixture constraints"},{"location":"nchoosek_constraint/","text":"Tentative NChooseK constraint support doe also supports problems with NChooseK constraints. Since IPOPT has problems finding feasible solutions using the gradient of the NChooseK constraint violation, a closely related (but stricter) constraint that suffices to fulfill the NChooseK constraint is imposed onto the problem: For each experiment \\(j\\) N-K decision variables \\(x_{i_1,j},...,x_{i_{N-K,j}}\\) from the NChooseK constraints' names attribute are picked that are forced to be zero. This is done by setting the upper and lower bounds of the picked variables are set to 0 in the corresponding experiments. This causes IPOPT to treat them as \"fixed variables\" (i.e. it will not optimize for them) and will always stick to the only feasible value (which is 0 here). However, this constraint is stricter than the original NChooseK constraint. In combination with other constraints on the same decision variables this can result in a situation where the constraints cannot be fulfilled even though the original constraints would allow for a solution. For example consider a problem with three decision variables \\(x_1, x_2, x_3, x_4\\) , an NChooseK constraint on all three variable that restricts the number of active constraints to two. Additionally, we have a linear constraint $$ x_3 + x_4 \\geq 0.1 $$ We can easily find points that fulfill both constraints (e.g. \\((0,0,0,0.1)\\) ). Now consider the stricter, linear constraint from above. Eventually, it will happen that \\(x_3\\) and \\(x_4\\) are chosen to be zero for one experiment. For this experiment it is impossible to fulfill the linear constraint \\(x_3 + x_4 \\geq 0.1\\) since \\(x_3 = x_4 = 0\\) . Therefore one has to be very careful when imposing linear constraints upon decision variables that already show up in an NChooseK constraint. For practical reasons it necessary that two NChooseK constraints of the same problem must not share any variables. You can find an example for a problem with NChooseK constraints and additional linear constraints imposed on the same variables. import opti from doe import find_local_max_ipopt problem = opti . Problem ( inputs = [ opti . Continuous ( f \"x { i + 1 } \" , [ 0 , 1 ]) for i in range ( 8 )], outputs = [ opti . Continuous ( \"y\" )], constraints = [ opti . LinearEquality ( names = [ f \"x { i + 1 } \" for i in range ( 8 )], rhs = 1 ), opti . NChooseK ( names = [ \"x1\" , \"x2\" , \"x3\" ], max_active = 1 ), opti . LinearInequality ( names = [ \"x1\" , \"x2\" , \"x3\" ], rhs = 0.7 ), opti . LinearInequality ( names = [ \"x7\" , \"x8\" ], lhs =- 1 , rhs =- 0.1 ), opti . LinearInequality ( names = [ \"x7\" , \"x8\" ], rhs = 0.9 ), ] ) res = find_local_max_ipopt ( problem = problem , model_type = \"fully-quadratic\" , ipopt_options = { \"maxiter\" : 500 , \"disp\" : 5 }, ) Running these lines of codes yields the following design. x1 x2 x3 x4 x5 x6 x7 x8 0.00 0.00 0.00 0.90 0.00 0.00 0.10 0.00 0.36 0.00 0.00 0.00 0.00 0.00 0.00 0.64 0.00 0.00 0.00 0.00 0.90 0.00 0.00 0.10 0.00 0.00 0.00 0.00 0.45 0.45 0.10 0.00 0.45 0.00 0.00 0.45 0.00 0.00 0.10 0.00 0.00 0.00 0.00 0.90 0.00 0.00 0.00 0.10 0.00 0.45 0.00 0.45 0.00 0.00 0.00 0.10 0.70 0.00 0.00 0.00 0.00 0.00 0.00 0.30 0.00 0.00 0.00 0.00 0.00 0.48 0.00 0.52 0.00 0.00 0.00 0.45 0.00 0.45 0.08 0.02 0.69 0.01 0.00 0.00 0.00 0.20 0.10 0.00 0.00 0.00 0.46 0.00 0.00 0.00 0.54 0.00 0.00 0.00 0.00 0.00 0.90 0.00 0.10 0.00 0.51 0.00 0.00 0.00 0.00 0.00 0.50 0.00 0.00 0.00 0.39 0.00 0.00 0.00 0.00 0.62 0.00 0.36 0.00 0.00 0.00 0.55 0.00 0.10 0.10 0.00 0.00 0.00 0.00 0.00 0.90 0.00 0.00 0.00 0.70 0.00 0.00 0.00 0.00 0.30 0.00 0.54 0.00 0.00 0.00 0.00 0.00 0.46 0.00 0.00 0.00 0.00 0.53 0.00 0.00 0.48 0.00 0.00 0.00 0.00 0.10 0.00 0.00 0.90 0.00 0.00 0.00 0.49 0.00 0.00 0.51 0.00 0.00 0.00 0.00 0.00 0.10 0.00 0.90 0.00 0.00 0.00 0.45 0.45 0.00 0.00 0.04 0.06 0.00 0.00 0.00 0.45 0.45 0.00 0.03 0.07 0.45 0.00 0.00 0.00 0.45 0.00 0.10 0.00 0.00 0.00 0.70 0.00 0.00 0.20 0.10 0.00 0.00 0.00 0.00 0.00 0.00 0.09 0.45 0.45 0.44 0.00 0.00 0.00 0.00 0.46 0.00 0.10 0.00 0.00 0.45 0.00 0.45 0.00 0.10 0.00 0.00 0.43 0.00 0.00 0.00 0.00 0.57 0.00 0.00 0.00 0.00 0.49 0.00 0.00 0.00 0.51 0.00 0.00 0.00 0.00 0.00 0.49 0.51 0.00 0.00 0.70 0.00 0.00 0.00 0.20 0.10 0.00 0.00 0.00 0.00 0.00 0.00 0.90 0.00 0.10 0.00 0.00 0.00 0.00 0.00 0.90 0.10 0.00 0.00 0.45 0.00 0.00 0.45 0.00 0.00 0.10 0.00 0.00 0.00 0.00 0.50 0.00 0.50 0.00 0.00 0.00 0.40 0.00 0.00 0.50 0.00 0.10","title":"Tentative NChooseK constraint support"},{"location":"nchoosek_constraint/#tentative-nchoosek-constraint-support","text":"doe also supports problems with NChooseK constraints. Since IPOPT has problems finding feasible solutions using the gradient of the NChooseK constraint violation, a closely related (but stricter) constraint that suffices to fulfill the NChooseK constraint is imposed onto the problem: For each experiment \\(j\\) N-K decision variables \\(x_{i_1,j},...,x_{i_{N-K,j}}\\) from the NChooseK constraints' names attribute are picked that are forced to be zero. This is done by setting the upper and lower bounds of the picked variables are set to 0 in the corresponding experiments. This causes IPOPT to treat them as \"fixed variables\" (i.e. it will not optimize for them) and will always stick to the only feasible value (which is 0 here). However, this constraint is stricter than the original NChooseK constraint. In combination with other constraints on the same decision variables this can result in a situation where the constraints cannot be fulfilled even though the original constraints would allow for a solution. For example consider a problem with three decision variables \\(x_1, x_2, x_3, x_4\\) , an NChooseK constraint on all three variable that restricts the number of active constraints to two. Additionally, we have a linear constraint $$ x_3 + x_4 \\geq 0.1 $$ We can easily find points that fulfill both constraints (e.g. \\((0,0,0,0.1)\\) ). Now consider the stricter, linear constraint from above. Eventually, it will happen that \\(x_3\\) and \\(x_4\\) are chosen to be zero for one experiment. For this experiment it is impossible to fulfill the linear constraint \\(x_3 + x_4 \\geq 0.1\\) since \\(x_3 = x_4 = 0\\) . Therefore one has to be very careful when imposing linear constraints upon decision variables that already show up in an NChooseK constraint. For practical reasons it necessary that two NChooseK constraints of the same problem must not share any variables. You can find an example for a problem with NChooseK constraints and additional linear constraints imposed on the same variables. import opti from doe import find_local_max_ipopt problem = opti . Problem ( inputs = [ opti . Continuous ( f \"x { i + 1 } \" , [ 0 , 1 ]) for i in range ( 8 )], outputs = [ opti . Continuous ( \"y\" )], constraints = [ opti . LinearEquality ( names = [ f \"x { i + 1 } \" for i in range ( 8 )], rhs = 1 ), opti . NChooseK ( names = [ \"x1\" , \"x2\" , \"x3\" ], max_active = 1 ), opti . LinearInequality ( names = [ \"x1\" , \"x2\" , \"x3\" ], rhs = 0.7 ), opti . LinearInequality ( names = [ \"x7\" , \"x8\" ], lhs =- 1 , rhs =- 0.1 ), opti . LinearInequality ( names = [ \"x7\" , \"x8\" ], rhs = 0.9 ), ] ) res = find_local_max_ipopt ( problem = problem , model_type = \"fully-quadratic\" , ipopt_options = { \"maxiter\" : 500 , \"disp\" : 5 }, ) Running these lines of codes yields the following design. x1 x2 x3 x4 x5 x6 x7 x8 0.00 0.00 0.00 0.90 0.00 0.00 0.10 0.00 0.36 0.00 0.00 0.00 0.00 0.00 0.00 0.64 0.00 0.00 0.00 0.00 0.90 0.00 0.00 0.10 0.00 0.00 0.00 0.00 0.45 0.45 0.10 0.00 0.45 0.00 0.00 0.45 0.00 0.00 0.10 0.00 0.00 0.00 0.00 0.90 0.00 0.00 0.00 0.10 0.00 0.45 0.00 0.45 0.00 0.00 0.00 0.10 0.70 0.00 0.00 0.00 0.00 0.00 0.00 0.30 0.00 0.00 0.00 0.00 0.00 0.48 0.00 0.52 0.00 0.00 0.00 0.45 0.00 0.45 0.08 0.02 0.69 0.01 0.00 0.00 0.00 0.20 0.10 0.00 0.00 0.00 0.46 0.00 0.00 0.00 0.54 0.00 0.00 0.00 0.00 0.00 0.90 0.00 0.10 0.00 0.51 0.00 0.00 0.00 0.00 0.00 0.50 0.00 0.00 0.00 0.39 0.00 0.00 0.00 0.00 0.62 0.00 0.36 0.00 0.00 0.00 0.55 0.00 0.10 0.10 0.00 0.00 0.00 0.00 0.00 0.90 0.00 0.00 0.00 0.70 0.00 0.00 0.00 0.00 0.30 0.00 0.54 0.00 0.00 0.00 0.00 0.00 0.46 0.00 0.00 0.00 0.00 0.53 0.00 0.00 0.48 0.00 0.00 0.00 0.00 0.10 0.00 0.00 0.90 0.00 0.00 0.00 0.49 0.00 0.00 0.51 0.00 0.00 0.00 0.00 0.00 0.10 0.00 0.90 0.00 0.00 0.00 0.45 0.45 0.00 0.00 0.04 0.06 0.00 0.00 0.00 0.45 0.45 0.00 0.03 0.07 0.45 0.00 0.00 0.00 0.45 0.00 0.10 0.00 0.00 0.00 0.70 0.00 0.00 0.20 0.10 0.00 0.00 0.00 0.00 0.00 0.00 0.09 0.45 0.45 0.44 0.00 0.00 0.00 0.00 0.46 0.00 0.10 0.00 0.00 0.45 0.00 0.45 0.00 0.10 0.00 0.00 0.43 0.00 0.00 0.00 0.00 0.57 0.00 0.00 0.00 0.00 0.49 0.00 0.00 0.00 0.51 0.00 0.00 0.00 0.00 0.00 0.49 0.51 0.00 0.00 0.70 0.00 0.00 0.00 0.20 0.10 0.00 0.00 0.00 0.00 0.00 0.00 0.90 0.00 0.10 0.00 0.00 0.00 0.00 0.00 0.90 0.10 0.00 0.00 0.45 0.00 0.00 0.45 0.00 0.00 0.10 0.00 0.00 0.00 0.00 0.50 0.00 0.50 0.00 0.00 0.00 0.40 0.00 0.00 0.50 0.00 0.10","title":"Tentative NChooseK constraint support"},{"location":"tips/","text":"Tips This is a collection of some experiences we made during development of this package. IPOPT and the find_local_max_ipopt function itself allow for custom settings of many parameters. A complete list of IPOPT options can be found here . IPOPT options can be passed to the optimizer as a dictionary via the ipopt_options argument. If you are not satisfied with the results you get, it might be worth a try playing around with one or more of these parameters. disp : If you do not know what caused your problems or you want to find out more about the behavior of the optimizer you can change the value of this IPOPT option. IPOPT allows for values between 0 and 12 where the level of verbosity is higher for higher values. For values larger than 12 a message is generated in find_local_max_ipopt and printed before termination of the function. maxiter : The maximum number of iterations performed by IPOPT. This is especially relevant for complicated problem, where the optimizer does not find an optimum in an appropriate amount of time. acceptable_tol : The situation can occur that IPOPT terminates at an unsatisfactory point with the message \"EXIT: Solved To Acceptable Level.\". In this case, it can help to set the value of this parameter to the same value as the IPOPT option tol (do not mix this with the find_local_max_ipopt argument tol ) or some other lower value. tol : This argument defines the size of a small \"safety margin\" around the set of feasible solution that will also be considered as feasible. Increasing this value can make it easier for the algorithm to find feasible solutions. However this also means a perturbation of the original problem/problem set definition. The default value is 0. delta : This is the regularization parameter for the information matrix. Setting it to a larger value means a stronger regularization. Note that this also means a larger perturbation of the objective function. sampling : By default find_local_max_ipopt() will try to use the sample_inputs() method of the opti problem passed to the function for sampling the initial values. However, you can also pass your own initial guess or use a different sampling strategy as sampling parameter.","title":"Tips"},{"location":"tips/#tips","text":"This is a collection of some experiences we made during development of this package. IPOPT and the find_local_max_ipopt function itself allow for custom settings of many parameters. A complete list of IPOPT options can be found here . IPOPT options can be passed to the optimizer as a dictionary via the ipopt_options argument. If you are not satisfied with the results you get, it might be worth a try playing around with one or more of these parameters. disp : If you do not know what caused your problems or you want to find out more about the behavior of the optimizer you can change the value of this IPOPT option. IPOPT allows for values between 0 and 12 where the level of verbosity is higher for higher values. For values larger than 12 a message is generated in find_local_max_ipopt and printed before termination of the function. maxiter : The maximum number of iterations performed by IPOPT. This is especially relevant for complicated problem, where the optimizer does not find an optimum in an appropriate amount of time. acceptable_tol : The situation can occur that IPOPT terminates at an unsatisfactory point with the message \"EXIT: Solved To Acceptable Level.\". In this case, it can help to set the value of this parameter to the same value as the IPOPT option tol (do not mix this with the find_local_max_ipopt argument tol ) or some other lower value. tol : This argument defines the size of a small \"safety margin\" around the set of feasible solution that will also be considered as feasible. Increasing this value can make it easier for the algorithm to find feasible solutions. However this also means a perturbation of the original problem/problem set definition. The default value is 0. delta : This is the regularization parameter for the information matrix. Setting it to a larger value means a stronger regularization. Note that this also means a larger perturbation of the objective function. sampling : By default find_local_max_ipopt() will try to use the sample_inputs() method of the opti problem passed to the function for sampling the initial values. However, you can also pass your own initial guess or use a different sampling strategy as sampling parameter.","title":"Tips"},{"location":"tutorial/","text":"Basic example In this tutorial we solve a bunch of simple DoE-problems and while doing so we explore the functions provided by this package. We start with the problem of finding a D-optimal design for fitting a linear model with three continuous decision variables \\[ x_1, x_2, x_3 \\in [0,1], \\] eight experiments and no additional constraints. The figure below shows a D-optimal design for this problem. Now let's try to find this design with doe. First, we have to define the problem in opti import opti problem = opti . Problem ( inputs = [ opti . Continuous ( \"x1\" , [ 0 , 1 ]), opti . Continuous ( \"x2\" , [ 0 , 1 ]), opti . Continuous ( \"x3\" , [ 0 , 1 ]), ], outputs = [ opti . Continuous ( \"y\" ) ], ) The output \\(y\\) is added for technical reasons, because every opti.Problem requires at least one output. But it will not play any role for the functionality of doe. Now we hand over the problem to doe's find_local_max_ipopt() function and see the results. import doe res = doe . find_local_max_ipopt ( problem = problem , model_type = \"linear\" , n_experiments = 8 , ) Nice, our method could reproduce the d-optimal design. Note that there are different ways to specify the model we refer to, namely as a string or as a Formula object. In our example we could write the model_type parameter as from formulaic import Formula model_type = Formula ( \"x1 + x2 + x3\" ) or just model_type = \"x1 + x2 + x3\" Also note that n_experiments is an optional paramter. If no value is given, find_local_max_ipopt() will automatically find a number of experiments that fits to the problem and model. Now let's go one step further and add constraints to the problem. We decide to add one mixture constraint and three additional linear inequality constraints. \\(x_1 + x_2 + x_3 = 1\\) \\(x_2 \\geq 0.1\\) \\(x_3 \\leq 0.6\\) \\(5 x_1 + 4 x_2 \\leq 3.9\\) \\(20 x_1 - 5 x_2 \\geq 3\\) Then the problem reads problem = opti . Problem ( inputs = opti . Parameters ([ opti . Continuous ( f \"x { i + 1 } \" , [ 0 , 1 ]) for i in range ( 3 )]), outputs = [ opti . Continuous ( \"y\" )], constraints = [ opti . LinearEquality ( names = [ \"x1\" , \"x2\" , \"x3\" ], rhs = 1 ), opti . LinearInequality ([ \"x2\" ], lhs = [ - 1 ], rhs =- 0.1 ), opti . LinearInequality ([ \"x3\" ], lhs = [ 1 ], rhs = 0.6 ), opti . LinearInequality ([ \"x1\" , \"x2\" ], lhs = [ 5 , 4 ], rhs = 3.9 ), opti . LinearInequality ([ \"x1\" , \"x2\" ], lhs = [ - 20 , 5 ], rhs =- 3 ) ] ) res = find_local_max_ipopt ( problem , \"linear\" ) As expected for linear models, find_local_max_ipopt finds the corner points of the feasible set JacobianForLogdet Internally, find_local_max_ipopt uses gradient based optimization methods from cyipopt . For larger models and multiple decision variables, the dimensionality of the problem quickly rises and gradient evaluation with finite differences takes a lot of time. Therefore, doe uses an analytic formulation for gradients of the objective and the constraints. However, for models with terms of higher than 3rd order, one part of the gradient has to be provided by the user (for a more detailed explanation, see the documentation of JacobianForLogdet ). The gradient of the objective function \\(\\log(\\det(I + \\delta \\mathbb{I}))\\) (where \\(I\\) is the information matrix and \\(\\delta\\) is a regularization parameter) is implemented as the class JacobianForLogdet . To create an instance one has to pass the opti problem, the model and the number of experiments, e.g. model = doe . get_formula_from_string ( problem = problem , model_type = \"linear\" ) J = doe . JacobianForLogdet ( problem = problem , model = model , n_experiments = 2 ) The jacobian at one point can then be evaluated using the method jacobian . x = np . array ([ 1 , 0 , 0 , 0 , 1 , 0 ]) model = doe . get_formula_from_string ( problem = problem , model_type = \"linear\" ) J . jacobian ( x ) >>> array ([ - 1.33333322 , 0.66666658 , 0. , 0.66666658 , - 1.33333322 , 0. ]) For models with terms of higher order than three, the user has to provide a function with signature jacobian_building_block(x: np.ndarray) -> nd.ndarray . It should take a decision variable vector as input and return a the matrix \\(\\Big(\\frac{\\partial y_i}{\\partial x_j}\\Big)_{ij}\\) where \\((y_i)_i\\) are the model terms and \\((x_j)_j\\) the decision variables. You can pass this function as an optional parameter when creating a JacobianForLogdet object. Internally, this is what find_local_max_ipopt does when you pass a function as the optional parameter jacobian_building_block .","title":"Tutorial"},{"location":"tutorial/#basic-example","text":"In this tutorial we solve a bunch of simple DoE-problems and while doing so we explore the functions provided by this package. We start with the problem of finding a D-optimal design for fitting a linear model with three continuous decision variables \\[ x_1, x_2, x_3 \\in [0,1], \\] eight experiments and no additional constraints. The figure below shows a D-optimal design for this problem. Now let's try to find this design with doe. First, we have to define the problem in opti import opti problem = opti . Problem ( inputs = [ opti . Continuous ( \"x1\" , [ 0 , 1 ]), opti . Continuous ( \"x2\" , [ 0 , 1 ]), opti . Continuous ( \"x3\" , [ 0 , 1 ]), ], outputs = [ opti . Continuous ( \"y\" ) ], ) The output \\(y\\) is added for technical reasons, because every opti.Problem requires at least one output. But it will not play any role for the functionality of doe. Now we hand over the problem to doe's find_local_max_ipopt() function and see the results. import doe res = doe . find_local_max_ipopt ( problem = problem , model_type = \"linear\" , n_experiments = 8 , ) Nice, our method could reproduce the d-optimal design. Note that there are different ways to specify the model we refer to, namely as a string or as a Formula object. In our example we could write the model_type parameter as from formulaic import Formula model_type = Formula ( \"x1 + x2 + x3\" ) or just model_type = \"x1 + x2 + x3\" Also note that n_experiments is an optional paramter. If no value is given, find_local_max_ipopt() will automatically find a number of experiments that fits to the problem and model. Now let's go one step further and add constraints to the problem. We decide to add one mixture constraint and three additional linear inequality constraints. \\(x_1 + x_2 + x_3 = 1\\) \\(x_2 \\geq 0.1\\) \\(x_3 \\leq 0.6\\) \\(5 x_1 + 4 x_2 \\leq 3.9\\) \\(20 x_1 - 5 x_2 \\geq 3\\) Then the problem reads problem = opti . Problem ( inputs = opti . Parameters ([ opti . Continuous ( f \"x { i + 1 } \" , [ 0 , 1 ]) for i in range ( 3 )]), outputs = [ opti . Continuous ( \"y\" )], constraints = [ opti . LinearEquality ( names = [ \"x1\" , \"x2\" , \"x3\" ], rhs = 1 ), opti . LinearInequality ([ \"x2\" ], lhs = [ - 1 ], rhs =- 0.1 ), opti . LinearInequality ([ \"x3\" ], lhs = [ 1 ], rhs = 0.6 ), opti . LinearInequality ([ \"x1\" , \"x2\" ], lhs = [ 5 , 4 ], rhs = 3.9 ), opti . LinearInequality ([ \"x1\" , \"x2\" ], lhs = [ - 20 , 5 ], rhs =- 3 ) ] ) res = find_local_max_ipopt ( problem , \"linear\" ) As expected for linear models, find_local_max_ipopt finds the corner points of the feasible set","title":"Basic example"},{"location":"tutorial/#jacobianforlogdet","text":"Internally, find_local_max_ipopt uses gradient based optimization methods from cyipopt . For larger models and multiple decision variables, the dimensionality of the problem quickly rises and gradient evaluation with finite differences takes a lot of time. Therefore, doe uses an analytic formulation for gradients of the objective and the constraints. However, for models with terms of higher than 3rd order, one part of the gradient has to be provided by the user (for a more detailed explanation, see the documentation of JacobianForLogdet ). The gradient of the objective function \\(\\log(\\det(I + \\delta \\mathbb{I}))\\) (where \\(I\\) is the information matrix and \\(\\delta\\) is a regularization parameter) is implemented as the class JacobianForLogdet . To create an instance one has to pass the opti problem, the model and the number of experiments, e.g. model = doe . get_formula_from_string ( problem = problem , model_type = \"linear\" ) J = doe . JacobianForLogdet ( problem = problem , model = model , n_experiments = 2 ) The jacobian at one point can then be evaluated using the method jacobian . x = np . array ([ 1 , 0 , 0 , 0 , 1 , 0 ]) model = doe . get_formula_from_string ( problem = problem , model_type = \"linear\" ) J . jacobian ( x ) >>> array ([ - 1.33333322 , 0.66666658 , 0. , 0.66666658 , - 1.33333322 , 0. ]) For models with terms of higher order than three, the user has to provide a function with signature jacobian_building_block(x: np.ndarray) -> nd.ndarray . It should take a decision variable vector as input and return a the matrix \\(\\Big(\\frac{\\partial y_i}{\\partial x_j}\\Big)_{ij}\\) where \\((y_i)_i\\) are the model terms and \\((x_j)_j\\) the decision variables. You can pass this function as an optional parameter when creating a JacobianForLogdet object. Internally, this is what find_local_max_ipopt does when you pass a function as the optional parameter jacobian_building_block .","title":"JacobianForLogdet"}]}